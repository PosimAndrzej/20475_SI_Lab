{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b13427a-f316-48ff-9468-964060af4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from dotenv import load_dotenv \n",
    "from bs4 import BeautifulSoup \n",
    "from IPython.display import Markdown, display \n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6c73b7-ff37-46db-a90b-282e1c050499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer technology is a fascinating field that has revolutionized the way we transmit and distribute electrical energy. Here's an overview:\n",
      "\n",
      "**What is a Transformer?**\n",
      "\n",
      "A transformer is an electrical device that transfers electrical energy from one circuit to another through electromagnetic induction. It consists of two coils of wire, known as primary and secondary coils, which are wrapped around a common magnetic core.\n",
      "\n",
      "**How does it work?**\n",
      "\n",
      "When an alternating current (AC) flows through the primary coil, it generates a magnetic field. This magnetic field induces a voltage in the secondary coil, creating an electromotive force (EMF). The ratio of the voltages in the primary and secondary coils is determined by the number of turns in each coil.\n",
      "\n",
      "**Key Benefits of Transformers:**\n",
      "\n",
      "1. **Efficient Energy Transfer**: Transformers allow for efficient transfer of electrical energy over long distances with minimal energy loss.\n",
      "2. **Voltage Transformation**: Transformers can step up or step down the voltage of an electrical signal, making it suitable for various applications.\n",
      "3. **Isolation and Safety**: Transformers provide electrical isolation between the primary and secondary circuits, reducing the risk of electrical shock and improving safety.\n",
      "\n",
      "**Types of Transformers:**\n",
      "\n",
      "1. **Step-up Transformers**: Used to increase the voltage of an electrical signal.\n",
      "2. **Step-down Transformers**: Used to decrease the voltage of an electrical signal.\n",
      "3. **Isolation Transformers**: Used for electrical isolation between two circuits.\n",
      "4. **Autotransformers**: A type of transformer that uses a single coil for both primary and secondary circuits.\n",
      "\n",
      "**Modern Applications:**\n",
      "\n",
      "1. **Power Grids**: Transformers are used extensively in power grids to transmit electricity over long distances with minimal energy loss.\n",
      "2. **Industrial Power Systems**: Transformers are used in industrial settings to provide power to machinery and equipment.\n",
      "3. **Medical Equipment**: Transformers are used in medical equipment, such as X-ray machines and defibrillators.\n",
      "\n",
      "**Future Trends:**\n",
      "\n",
      "1. **High-Voltage Direct Current (HVDC) Transmission**: Transformers are being developed for HVDC transmission systems, which can transmit electricity over long distances with minimal energy loss.\n",
      "2. **Advanced Materials**: Researchers are exploring the use of advanced materials, such as silicon carbide and graphene, to improve transformer efficiency and reliability.\n",
      "\n",
      "In summary, transformer technology is a vital component of modern power systems, enabling efficient energy transfer and voltage transformation while providing electrical isolation and safety.\n"
     ]
    }
   ],
   "source": [
    "message = \"Tell me something about transformer technology\" \n",
    "response = ollama.chat(model='llama3.2', messages=[{\"role\":\"user\", \"content\":message}]) \n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cac3b936-9447-4311-b075-241f9160eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\" \n",
    "}\n",
    "class Website: \n",
    "    def __init__(self, url): \n",
    "        self.url = url \n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') \n",
    "        self.title = soup.title.string if soup.title else \"No title found\" \n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\", \"link\", \"ul\", \"11\", \"a\"]): \n",
    "            irrelevant.decompose() \n",
    "        self.text = soup.body.get_text (separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a514e92d-f59d-41a1-a82e-4859a212f25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Duży model językowy – Wikipedia, wolna encyklopedia\n",
      "text:  Menu główne\n",
      "Menu główne\n",
      "przypnij\n",
      "ukryj\n",
      "Nawigacja\n",
      "Dla czytelników\n",
      "Dla wikipedystów\n",
      "Szukaj\n",
      "Wygląd\n",
      "Narzędzia osobiste\n",
      "Strony dla anonimowych edytorów\n",
      "Spis treści\n",
      "przypnij\n",
      "ukryj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n",
      "polski\n",
      "Narzędzia\n",
      "Narzędzia\n",
      "przypnij\n",
      "ukryj\n",
      "Działania\n",
      "Ogólne\n",
      "Drukuj lub eksportuj\n",
      "W innych projektach\n",
      "Wygląd\n",
      "przypnij\n",
      "ukryj\n",
      "Z Wikipedii, wolnej encyklopedii\n",
      "Duży model językowy\n",
      "(ang.\n",
      "large language model\n",
      ",\n",
      "LLM\n",
      ")\n",
      "– model\n",
      "umożliwiający generowanie tekstu oraz realizację zadań związanych z\n",
      ". Modele\n",
      "LLM\n",
      "są szkolone w ramach\n",
      "lub słabo nadzorowanego\n",
      "z wykorzystaniem dużych ilości danych tekstowych. Proces ten jest intensywny obliczeniowo\n",
      ". Duże modele językowe mogą być wykorzystywane do generowana tekstu poprzez wielokrotne przewidywanie następnego tokenu lub słowa, przez co zaliczane są do\n",
      ".\n",
      "Duże modele językowe są\n",
      ". Największe i najbardziej zdolne modele językowe oparte są na architekturze\n",
      ".\n",
      "Przykładami dużych modeli językowych są modele z serii\n",
      "zbudowane przez\n",
      "(np.\n",
      "﻿\n",
      ",\n",
      "﻿\n",
      "), używane w chatbotach\n",
      "i Microsoft Copilot, a także modele\n",
      "zbudowane przez\n",
      ". Istnieją również chińskie modele jak\n",
      "czy polskie jak\n",
      "i\n",
      ".\n",
      "Historia\n",
      "[\n",
      "|\n",
      "]\n",
      "Przed rokiem 2017 istniało kilka modeli językowych, które — jak na ówczesne możliwości — były uważane za duże. W latach 90.\n",
      "pozwoliły na rozwinięcie się metod statystycznego modelowania języka. W 2001 roku model bazujący na\n",
      "został wytrenowany na 300 milionach słów\n",
      ". W latach dwutysięcznych, wraz z popularyzacją\n",
      ", rozpoczęto tworzenie zbiorów językowych o skali porównywalnej z Internetem\n",
      ", na bazie których podjęto próby uczenia statystycznych modeli językowych\n",
      ".\n",
      "Po tym, jak sieci neuronowe stały się popularne w przetwarzaniu obrazów około 2012 roku\n",
      ", zaczęto je również stosować do przetwarzania tekstu. W 2016 roku Google wprowadziło w\n",
      "swój model językowy oparty na\n",
      ".\n",
      "Wykres pokazujący łączną ilość obliczeń (we\n",
      "-sach) wymaganą do wytrenowania modelu w kolejnych latach dla wybranych dużych modeli AI. Większość dużych modeli to modele językowe lub mutlimodalne ze zdolnościami przetwarzania języka.\n",
      "W 2017 roku naukowcy z Google zaproponowali architekturę\n",
      "opartą na\n",
      "opracowanym w 2014 roku\n",
      ". W 2018 roku Google wypuściło model BERT oparty wyłącznie na\n",
      ". Od 2023 roku akademickie i badawcze zainteresowanie BERT-em zaczęło stopniowo maleć na rzecz modeli opartych na\n",
      "jak\n",
      ".\n",
      "Od 2022 roku zyskują na popularności modele o otwartym kodzie źródłowym – początkowo za sprawą projektów takich jak BLOOM\n",
      "i\n",
      ", choć oba objęte są ograniczeniami dotyczącymi zakresu zastosowań. Modele\n",
      "udostępnione zostały na bardziej liberalnej licencji\n",
      ". W styczniu 2025 roku firma\n",
      "wypuściła w formie otwartego kodu model DeepSeek R1, mający 671 miliardów parametrów. Jego wydajność jest porównywalna z modelem OpenAI o1, przy znacznie niższych kosztach eksploatacji\n",
      ".\n",
      "Od 2023 roku wiele modeli ma charakter\n",
      ", co oznacza, że potrafią analizować i generować różne typy danych, takie jak tekst, obrazy czy dźwięk\n",
      ".\n",
      "W roku 2024 największe i najbardziej zaawansowane modele językowe oparte są na architekturze transformatora. Niektóre nowsze implementacje wykorzystują jednak inne podejścia, takie jak\n",
      "(RNN) czy\n",
      "﻿\n",
      ".\n",
      "Trenowanie i architektura\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Wzmacnianie z informacją zwrotną od człowieka\n",
      "[\n",
      "|\n",
      "]\n",
      "to metoda umożliwiająca dostosowanie działania modelu językowego do ludzkich oczekiwań. Preferencje użytkowników definiuje się poprzez trenowanie tzw. modelu nagród, który następnie służy do dalszego uczenia modelu językowego z wykorzystaniem algorytmów\n",
      "takich jak\n",
      "proximal policy optimization\n",
      "(PPO)\n",
      ".\n",
      "Mieszanka ekspertów\n",
      "[\n",
      "|\n",
      "]\n",
      "Zobacz też:\n",
      ".\n",
      "Największe modele językowe bywają zbyt kosztowne w trenowaniu i bezpośrednim zastosowaniu. Dlatego coraz częściej stosuje się podejście mieszanki ekspertów (ang. Mixture of Experts, MoE)\n",
      ". MoE to technika, która dzieli przestrzeń problemu między wiele wyspecjalizowanych\n",
      "przez co inferencja aktywuje tylko te części sieci, które są najbardziej odpowiednie\n",
      ", a sama metoda jest zaliczana do metod\n",
      ".\n",
      "Dostrajanie na podstawie instrukcji\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe uczą się generować poprawne odpowiedzi i zastępować naiwne uzupełnienia dzięki kilku wstępnym korektom wprowadzonym przez człowieka oraz zastosowaniu podejścia\n",
      "self‑instruct\n",
      ". Na przykład w odpowiedzi na polecenie: „\n",
      "Napisz esej na temat głównych motywów przedstawionych w Hamlecie\n",
      "” model mógłby najpierw wygenerować: „\n",
      "Jeśli oddasz esej po 17 marca, Twoja ocena zostanie obniżona o 10% za każdy dzień opóźnienia\n",
      "”, bazując na częstotliwości takiego ciągu w\n",
      ".\n",
      "Koszt\n",
      "[\n",
      "|\n",
      "]\n",
      "Szacunkowy koszt trenowania wybranych modeli AI\n",
      "Trenowanie i działanie dużych modeli językowych zazwyczaj wymaga ogromnej\n",
      "i zużycia\n",
      ", co rodzi pytania dotyczące wpływu na\n",
      ".\n",
      "Rozumowanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Pod koniec 2024 roku w rozwoju dużych modeli językowych pojawił się nowy kierunek, skoncentrowany na zadaniach wymagających złożonego\n",
      ". Modele tego typu, określane jako „modele rozumujące”, zostały wytrenowane tak, aby poświęcać więcej czasu na generowanie rozwiązań krok po kroku (ang. chain-of-thought) przed udzieleniem odpowiedzi końcowej – w sposób zbliżony do ludzkiego procesu rozwiązywania problemów\n",
      ". Trend ten zapoczątkowała firma OpenAI, wprowadzając we wrześniu 2024 roku model o1\n",
      ", a następnie model o3 w grudniu 2024\n",
      ". W porównaniu z tradycyjnymi LLM-ami, nowe modele wykazywały znaczną poprawę wyników w zadaniach z matematyki, nauk ścisłych oraz programowania. Przykładowo, na zadaniach z eliminacji do\n",
      "model GPT-4o uzyskał 13% skuteczności, podczas gdy model o1 osiągnął aż 83%\n",
      ".\n",
      "W styczniu 2025 roku chińska firma\n",
      "przedstawiła model DeepSeek‑R1 — model rozumowania o otwartych wagach, posiadający 671 miliardów parametrów, który osiągnął wyniki porównywalne do modelu o1 firmy OpenAI, przy znacznie niższych kosztach operacyjnych. W przeciwieństwie do zastrzeżonych modeli OpenAI, otwarta architektura DeepSeek‑R1 umożliwiła badaczom analizę i dalszy rozwój algorytmu, choć dane treningowe pozostały nieupublicznione\n",
      ".\n",
      "Tego typu podejście z reguły mają większe wymagania obliczeniowe w porównaniu z bezpośrednim podejściem ponieważ model musi generować wiele odpowiedzi dla każdego kroku jednak pozwala to na osiągnięcie lepszych wyników w dziedzinach wymagających myślenia domenowego\n",
      ". Aby zmniejszyć ilość występowania\n",
      ", stosowane są dodatkowe techniki jak\n",
      ",\n",
      "czy\n",
      ".\n",
      "Oddziaływanie\n",
      "[\n",
      "|\n",
      "]\n",
      "Istnieją opinie twierdzące, że nie ma możliwości rozróżnienia tekstu stworzonego przez duży model językowy i przez człowieka\n",
      ". Goldman Sachs w 2023 roku twierdził, że LLM-y są w stanie zwiększyć globalne\n",
      "o 7% w ciągu dekady i bedą potrafiły wystawić na automatyzację pracę 300 mln osób\n",
      ".\n",
      "Prawa autorskie\n",
      "[\n",
      "|\n",
      "]\n",
      "W roku 2023, do sądów w Stanach Zjednoczonych wpłynęło kilka wniosków podważających używanie danych chronionych\n",
      "do trenowania modeli językowych z obrońcami opierającymi się na instytucję\n",
      ".\n",
      "Bezpieczeństwo\n",
      "[\n",
      "|\n",
      "]\n",
      "Duże modele językowe mogą być używane do tworzenia\n",
      ", w sposób świadomy lub nie lub do innych celów\n",
      ". Dostępność dużych modeli językowych może pozwolić na obniżenie poziom umiejętności wymaganych do popełnienia czynów\n",
      ".\n",
      "Dodatkowo, istnieje możliwość osadzenia uśpionych\n",
      ", czyli ukrytych funkcjonalności, które w normalnych warunkach nie wykonują akcji, a po uzyskaniu impulsu aktywującego, rozpoczynają wykonywania szkodliwych działań\n",
      ".\n",
      "Aplikacje LLM zawierają odpowiednie filtry moderacyjne jednak nie są one w pełni efektywne i pozwalają na wykorzystanie jako\n",
      "czy różnych nielegalnych operacji\n",
      ".\n",
      "Stronniczość algorytmiczna\n",
      "[\n",
      "|\n",
      "]\n",
      "Podczas gdy duże modele językowe są w stanie generować tekst przypominający ludzki, są skłonne do dziedziczenia i powiększania stronniczości zawartej w\n",
      ". Stronniczość może się objawiać w błędnym i niesprawiedliwym traktowaniu różnych grup demograficznych\n",
      ".\n",
      "Zobacz też\n",
      "[\n",
      "|\n",
      "]\n",
      "Przypisy\n",
      "[\n",
      "|\n",
      "]\n",
      "A short history of AI\n",
      ". „\n",
      "The Economist\n",
      "”, s. 56, 20th July 2024.\n",
      "OpenAI\n",
      ":\n",
      ". [dostęp 2024-05-08]. [zarchiwizowane z\n",
      "(2020-12-19)].\n",
      "[online], Deloitte Polska\n",
      "[dostęp 2024-05-08]\n",
      "(\n",
      "pol.\n",
      ")\n",
      ".\n",
      "Joshua\n",
      "J.\n",
      "Goodman\n",
      "Joshua\n",
      "J.\n",
      ",\n",
      ", arXiv, 9 sierpnia 2001,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "Adam\n",
      "A.\n",
      "Kilgarriff\n",
      "Adam\n",
      "A.\n",
      ",\n",
      "Gregory\n",
      "G.\n",
      "Grefenstette\n",
      "Gregory\n",
      "G.\n",
      ",\n",
      ", „Computational Linguistics”, 29 (3),\n",
      "2003\n",
      ", s. 333–347,\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "Michele\n",
      "M.\n",
      "Banko\n",
      "Michele\n",
      "M.\n",
      ",\n",
      "Eric\n",
      "E.\n",
      "Brill\n",
      "Eric\n",
      "E.\n",
      ",\n",
      ", ACL '01, USA: Association for Computational Linguistics, 6 lipca 2001, s. 26–33,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "p?,\n",
      "Philip\n",
      "P.\n",
      "Resnik\n",
      "Philip\n",
      "P.\n",
      ",\n",
      "Noah A.\n",
      "N.A.\n",
      "Smith\n",
      "Noah A.\n",
      "N.A.\n",
      ",\n",
      ", „Computational Linguistics”, 29 (3),\n",
      "2003\n",
      ", s. 349–380,\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "Chen\n",
      "i inni\n",
      ",\n",
      ", „Remote Sensing”, 13 (22),\n",
      "2021\n",
      ",\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      "[zarchiwizowane z\n",
      "2025-03-07]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Vaswani\n",
      "i inni\n",
      ",\n",
      "[online], Advances in Neural Information Processing Systems. 30. Curran Associates, Inc, 2017\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Dzmitry\n",
      "D.\n",
      "Bahdanau\n",
      "Dzmitry\n",
      "D.\n",
      ",\n",
      "Kyunghyun\n",
      "K.\n",
      "Cho\n",
      "Kyunghyun\n",
      "K.\n",
      ",\n",
      "Yoshua\n",
      "Y.\n",
      "Bengio\n",
      "Yoshua\n",
      "Y.\n",
      ",\n",
      ", arXiv, 19 maja 2016,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "Jacob\n",
      "J.\n",
      "Devlin\n",
      "Jacob\n",
      "J.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 24 maja 2019,\n",
      ":\n",
      "[dostęp 2025-04-15]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "[online], research.google\n",
      "[dostęp 2025-04-15]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Rajiv\n",
      "R.\n",
      "Movva\n",
      "Rajiv\n",
      "R.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 28 kwietnia 2024,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "[online], huggingface.co\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "Shubham\n",
      "S.\n",
      "Sharma\n",
      "Shubham\n",
      "S.\n",
      ",\n",
      "[online], VentureBeat, 20 stycznia 2025\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Dr Tehseen\n",
      "D.T.\n",
      "Zia\n",
      "Dr Tehseen\n",
      "D.T.\n",
      ",\n",
      "[online], Unite.AI, 8 stycznia 2024\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Dr Tehseen\n",
      "D.T.\n",
      "Zia\n",
      "Dr Tehseen\n",
      "D.T.\n",
      ",\n",
      "[online], Unite.AI, 8 stycznia 2024\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Long\n",
      "L.\n",
      "Ouyang\n",
      "Long\n",
      "L.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 4 marca 2022,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "Noam\n",
      "N.\n",
      "Shazeer\n",
      "Noam\n",
      "N.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 23 stycznia 2017,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "Dmitry\n",
      "D.\n",
      "Lepikhin\n",
      "Dmitry\n",
      "D.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 30 czerwca 2020,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "[online], research.google\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Tara\n",
      "T.\n",
      "Baldacchino\n",
      "Tara\n",
      "T.\n",
      "i inni\n",
      ",\n",
      ", „Mechanical Systems and Signal Processing”, 66, 2016, s. 178–200,\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "1 stycznia\n",
      ", www.worldscientific.com,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Yizhong\n",
      "Y.\n",
      "Wang\n",
      "Yizhong\n",
      "Y.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 25 maja 2023,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "[online], feb.net.pl\n",
      "[dostęp 2025-02-13]\n",
      ".\n",
      "Anna\n",
      "A.\n",
      "Klee-Bylica\n",
      "Anna\n",
      "A.\n",
      ",\n",
      "[online], Zielony blog WSIiZ, 16 maja 2024\n",
      "[dostęp 2025-02-13]\n",
      ".\n",
      "↑\n",
      "[online], openai.com\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "↑\n",
      "Cade\n",
      "C.\n",
      "Metz\n",
      "Cade\n",
      "C.\n",
      ",\n",
      ", „\n",
      "”, 20 grudnia 2024\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Brak numerów stron w czasopiśmie\n",
      "Elizabeth\n",
      "E.\n",
      "Gibney\n",
      "Elizabeth\n",
      "E.\n",
      ",\n",
      ", „\n",
      "”, 638 (8049),\n",
      "2025\n",
      ", s. 13–14,\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Belle\n",
      "B.\n",
      "Lin\n",
      "Belle\n",
      "B.\n",
      ",\n",
      ", „Wall Street Journal”, 5 lutego 2025\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Brak numerów stron w czasopiśmie\n",
      ", „Nature Biomedical Engineering”, 7 (2),\n",
      "2023\n",
      ", s. 85–86,\n",
      ":\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      ", „\n",
      "”\n",
      ",\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "Brak numerów stron w czasopiśmie\n",
      "[online], www.goldmansachs.com\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "[online], natlawreview.com\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Davey\n",
      "D.\n",
      "Alba\n",
      "Davey\n",
      "D.\n",
      ",\n",
      "[online], The Japan Times, 1 maja 2023\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "[online], www.science.org\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Evan\n",
      "E.\n",
      "Hubinger\n",
      "Evan\n",
      "E.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 17 stycznia 2024,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "Daniel\n",
      "D.\n",
      "Kang\n",
      "Daniel\n",
      "D.\n",
      "i inni\n",
      ",\n",
      ", arXiv, 11 lutego 2023,\n",
      ":\n",
      "[dostęp 2025-04-11]\n",
      ".\n",
      "url\n",
      "Brak numerów stron w książce\n",
      "[online], IACR ePrint 2024/586, 2024\n",
      ".\n",
      "Chris\n",
      "Ch.\n",
      "Stokel-Walker\n",
      "Chris\n",
      "Ch.\n",
      ",\n",
      "[online], Scientific American\n",
      "[dostęp 2025-04-11]\n",
      "(\n",
      "ang.\n",
      ")\n",
      ".\n",
      "Linki zewnętrzne\n",
      "[\n",
      "|\n",
      "]\n",
      "Pojęcia\n",
      "Aplikacje\n",
      "Implementacje\n",
      "Tekst do obrazu\n",
      "Tekst do wideo\n",
      "Inne\n",
      "Architektury\n",
      "(\n",
      "model językowy\n",
      "):\n",
      ":\n",
      "Źródło: „\n",
      "”\n",
      ":\n",
      "Ukryte kategorie:\n",
      "Szukaj\n",
      "Szukaj\n",
      "Przełącz stan spisu treści\n",
      "Duży model językowy\n",
      "55 języków\n"
     ]
    }
   ],
   "source": [
    "web = Website(\"https://pl.wikipedia.org/wiki/Du%C5%BCy_model_j%C4%99zykowy\") \n",
    "print(\"Title: \", web.title) \n",
    "print(\"text: \", web.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81d2d1c0-8347-4a0e-b1e2-609a1079669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related.\\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28595bb4-0bc9-406e-bc78-2bdcaa32d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website): \n",
    "    user_prompt = \"You are looking at a website titled (website.title)\" \n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\" \n",
    "    user_prompt += website.text \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e7567df-92f2-441c-ade6-11a56210e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def messages_for(website): \n",
    "    return [ \n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a509a4f-5be7-4efe-8cc9-c46b1e55212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Summary of (website.title)\\n\\nThe website appears to be a Polish-language weather forecasting service. The main content includes:\\n\\n### News and Announcements\\n- Justyna Steczkowska\\'s chance at the top 3 in the Eurovision final is being debated among users, with an option to cast votes and see results.\\n\\n### Weather Forecasting\\nThe website offers a \"Sprawdzamy pogodę\" (Polish for \"We check the weather\") feature. However, it seems that there was a technical issue causing problems with displaying the page at the time of my analysis.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL='llama3.2' \n",
    "def summarize(url): \n",
    "    website = Website(url) \n",
    "    response = ollama.chat( \n",
    "        model=MODEL, \n",
    "        messages=messages_for(website) \n",
    "    ) \n",
    "    return response['message']['content']\n",
    "summarize(\"https://wp.pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5902829c-bd4f-4d69-a3c3-18727a65c0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary of (website.title)\n",
       "\n",
       "* The website appears to be related to weather forecasting, as indicated by the Polish phrase \"Sprawdzamy pogodę dla Ciebie...\"\n",
       "* It features a menu section with options such as REKLAMA and USŁUGI WP.\n",
       "* There is also an option for users to observe the channel's content on various platforms (SONDA).\n",
       "* The website mentions a recent issue with displaying a page, which may have caused inconvenience to users.\n",
       "* Notably, there is a news-related section discussing Justyna Steczkowska's chances of winning a spot in the TOP3 at the Eurowizji finale, allowing users to cast their votes and view results.\n",
       "\n",
       "### News/Announcements Summary\n",
       "\n",
       "* Justyna Steczkowska has been mentioned as a potential candidate for a spot in the TOP3 at the Eurowizji finale.\n",
       "* Users can vote for her to win this spot."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_summary(url): \n",
    "    summary = summarize(url) \n",
    "    display(Markdown (summary))\n",
    "\n",
    "display_summary(\"https://wp.pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93580aca-4fb6-465b-b109-fdfc600daf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
